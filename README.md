# 1st_ML_Project
ğŸš— ì¤‘ê³ ì°¨ ê°€ê²© ì˜ˆì¸¡ ëª¨ë¸ ê°œë°œ í”„ë¡œì íŠ¸
1. í”„ë¡œì íŠ¸ ê°œìš” (Overview)
ì´ í”„ë¡œì íŠ¸ëŠ” ë°ì´í„° ë¶„ì„ ë° ë¨¸ì‹ ëŸ¬ë‹ì˜ ê¸°ì´ˆë¥¼ ë‹¤ì§€ê¸° ìœ„í•œ ì²« ë²ˆì§¸ ê°œì¸ í”„ë¡œì íŠ¸ì…ë‹ˆë‹¤. Kaggleì˜ ì¤‘ê³ ì°¨ ë°ì´í„°ë¥¼ í™œìš©í•˜ì—¬, ì°¨ëŸ‰ì˜ ë‹¤ì–‘í•œ íŠ¹ì„±(í”¼ì²˜)ì„ ê¸°ë°˜ìœ¼ë¡œ ê°€ê²©ì„ ì˜ˆì¸¡í•˜ëŠ” íšŒê·€ ëª¨ë¸ì„ ê°œë°œí•˜ê³  í‰ê°€í•˜ëŠ” ì „ ê³¼ì •ì„ ë‹´ê³  ìˆìŠµë‹ˆë‹¤.

ë‹¨ìˆœíˆ ëª¨ë¸ì„ ë§Œë“œëŠ” ê²ƒì„ ë„˜ì–´, ë°ì´í„° ì •ì œë¶€í„° í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§, ëª¨ë¸ë§, ì„±ëŠ¥ í‰ê°€ì— ì´ë¥´ëŠ” ë¨¸ì‹ ëŸ¬ë‹ í”„ë¡œì íŠ¸ì˜ ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ ì§ì ‘ ê²½í—˜í•˜ê³  ì´í•´í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ë¬¸ì œ í•´ê²° ëŠ¥ë ¥ê³¼ ë°ì´í„° ê¸°ë°˜ì˜ ë…¼ë¦¬ì  ì‚¬ê³  ì—­ëŸ‰ì„ ì¦ëª…í•˜ê³ ì í•©ë‹ˆë‹¤.

2. ê°œë°œ í™˜ê²½ ë° ì‚¬ìš© ë°ì´í„°
ê°œë°œ í™˜ê²½: Python, Jupyter Notebook, Google Colab

ì£¼ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬: Pandas, NumPy, Matplotlib, Seaborn, Scikit-learn

ì‚¬ìš© ë°ì´í„°: Kaggle - Used Car Price Prediction (ì‚¬ìš©í•œ ë°ì´í„°ì…‹ì˜ ì •í™•í•œ ë§í¬ë¥¼ ì—¬ê¸°ì— ì‚½ì…í•˜ì„¸ìš”)

3. í”„ë¡œì íŠ¸ íŒŒì´í”„ë¼ì¸ (Methodology)
ë³¸ í”„ë¡œì íŠ¸ëŠ” ì•„ë˜ì™€ ê°™ì€ ì²´ê³„ì ì¸ ë‹¨ê³„ë¡œ ì§„í–‰ë˜ì—ˆìŠµë‹ˆë‹¤.

ë°ì´í„° íƒìƒ‰ (EDA): ë°ì´í„°ì˜ ê¸°ë³¸ êµ¬ì¡°ì™€ í†µê³„ì  íŠ¹ì„±ì„ íŒŒì•…í•˜ê³ , ë³€ìˆ˜ ê°„ì˜ ê´€ê³„ë¥¼ ì‹œê°í™”ë¥¼ í†µí•´ íƒìƒ‰í–ˆìŠµë‹ˆë‹¤.

ë°ì´í„° ì „ì²˜ë¦¬: ëª¨ë¸ í•™ìŠµì— ë°©í•´ê°€ ë˜ëŠ” ê²°ì¸¡ì¹˜ë¥¼ ì²˜ë¦¬í•˜ê³ , ëª¨ë¸ì´ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ë¬¸ìí˜• ë°ì´í„°(ë²”ì£¼í˜•)ë¥¼ ìˆ«ìí˜•ìœ¼ë¡œ ë³€í™˜í–ˆìŠµë‹ˆë‹¤.

í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§: ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ ê¸°ì¡´ í”¼ì²˜ë¥¼ ì¡°í•©í•˜ì—¬ 'ì°¨ëŸ‰ ë‚˜ì´(Car_Age)'ì™€ ê°™ì€ ìƒˆë¡œìš´ ì˜ë¯¸ë¥¼ ê°€ì§„ íŒŒìƒ ë³€ìˆ˜ë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤.

ëª¨ë¸ë§: ì„ í˜• íšŒê·€, ëœë¤ í¬ë ˆìŠ¤íŠ¸ ë“± ë‹¤ì–‘í•œ íšŒê·€ ëª¨ë¸ì„ í•™ìŠµí•˜ê³  ì„±ëŠ¥ì„ ë¹„êµ ë¶„ì„í–ˆìŠµë‹ˆë‹¤.

í‰ê°€: RMSE(Root Mean Squared Error)ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ê° ëª¨ë¸ì˜ ì˜ˆì¸¡ ì„±ëŠ¥ì„ ê°ê´€ì ìœ¼ë¡œ í‰ê°€í•˜ê³  ìµœì ì˜ ëª¨ë¸ì„ ì„ ì •í–ˆìŠµë‹ˆë‹¤.

4. í•µì‹¬ ê²°ê³¼ (Results)
ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ
ë‹¤ì–‘í•œ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¨ ê²°ê³¼, ë‹¤ìŒê³¼ ê°™ì€ ì„±ëŠ¥ì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤.

Model	RMSE (Root Mean Squared Error)
Linear Regression	(ì˜ˆ: 4.51)
Random Forest	(ì˜ˆ: 2.87)
Gradient Boosting	(ì˜ˆ: 2.55)

Sheetsë¡œ ë‚´ë³´ë‚´ê¸°
ë¶„ì„ ê²°ê³¼, Gradient Boosting ëª¨ë¸ì´ ê°€ì¥ ë‚®ì€ ì˜¤ë¥˜ìœ¨(RMSE)ì„ ë³´ì—¬ ìµœì  ëª¨ë¸ë¡œ ì„ ì •ë˜ì—ˆìŠµë‹ˆë‹¤.

ì£¼ìš” í”¼ì²˜ ì¤‘ìš”ë„
ìµœì¢… ëª¨ë¸ì¸ Gradient Boostingì—ì„œ í™•ì¸í•œ ì£¼ìš” í”¼ì²˜ ì¤‘ìš”ë„ì…ë‹ˆë‹¤. 'ì—”ì§„ ì¶œë ¥(Power)', 'ì°¨ëŸ‰ ë‚˜ì´(Car_Age)' ë“±ì´ ê°€ê²© ì˜ˆì¸¡ì— í° ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.

(ë…¸íŠ¸ë¶ì—ì„œ ë§Œë“  í”¼ì²˜ ì¤‘ìš”ë„ ê·¸ë˜í”„ ì´ë¯¸ì§€ë¥¼ ìº¡ì²˜í•˜ì—¬ ì—¬ê¸°ì— ì¶”ê°€í•˜ì„¸ìš”)

5. ê²°ë¡  ë° ë°°ìš´ ì  (Conclusion & What I Learned)
ê²°ë¡ 
ë³¸ í”„ë¡œì íŠ¸ë¥¼ í†µí•´ ì£¼í–‰ê±°ë¦¬, ì—°ì‹, ì—”ì§„ ì¶œë ¥ ë“± ì£¼ìš” í”¼ì²˜ë“¤ì´ ì¤‘ê³ ì°¨ ê°€ê²©ì— ì¤‘ìš”í•œ ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ê²ƒì„ ë°ì´í„° ê¸°ë°˜ìœ¼ë¡œ í™•ì¸í•  ìˆ˜ ìˆì—ˆìœ¼ë©°, ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì„±ê³µì ì¸ ì˜ˆì¸¡ ëª¨ë¸ì„ êµ¬ì¶•í–ˆìŠµë‹ˆë‹¤. íŠ¹íˆ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ì„ í†µí•´ 'ì°¨ëŸ‰ ë‚˜ì´'ì™€ ê°™ì€ ìƒˆë¡œìš´ ë³€ìˆ˜ë¥¼ ìƒì„±í•˜ëŠ” ê²ƒì´ ëª¨ë¸ ì„±ëŠ¥ì— í° ì˜í–¥ì„ ë¯¸ì¹œë‹¤ëŠ” ì ì„ ì‹¤ì¦ì ìœ¼ë¡œ í™•ì¸í–ˆìŠµë‹ˆë‹¤.

ë°°ìš´ ì  ë° í•œê³„
ë°°ìš´ ì :

ì´ë¡ ìœ¼ë¡œë§Œ ë°°ìš°ë˜ ë°ì´í„° ì „ì²˜ë¦¬(ê²°ì¸¡ì¹˜ ì²˜ë¦¬, ì¸ì½”ë”©)ê°€ ì‹¤ì œ ë¶„ì„ì—ì„œ ì–¼ë§ˆë‚˜ ì¤‘ìš”í•˜ê³  ë§ì€ ì‹œê°„ì„ ì°¨ì§€í•˜ëŠ”ì§€ ì²´ê°í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.

ë‹¨ìˆœíˆ ëª¨ë¸ì„ ì ìš©í•˜ëŠ” ê²ƒì„ ë„˜ì–´, EDAì™€ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ì„ í†µí•´ ë°ì´í„°ì— ëŒ€í•œ ê¹Šì€ ì´í•´ê°€ ì„ í–‰ë˜ì–´ì•¼ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ì œëŒ€ë¡œ ì´ëŒì–´ë‚¼ ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ ë°°ì› ìŠµë‹ˆë‹¤.

GitHubë¥¼ í†µí•´ í”„ë¡œì íŠ¸ ê³¼ì •ì„ ì²´ê³„ì ìœ¼ë¡œ ê¸°ë¡í•˜ê³  ë¬¸ì„œí™”í•˜ëŠ” ê²ƒì˜ ì¤‘ìš”ì„±ì„ ê¹¨ë‹¬ì•˜ìŠµë‹ˆë‹¤.

í•œê³„ì  ë° í–¥í›„ ê³¼ì œ (Future Work):

í˜„ì¬ ë°ì´í„°ì—ëŠ” ì°¨ëŸ‰ì˜ 'ì‚¬ê³  ìœ ë¬´', 'ìƒ‰ìƒ', 'ì„¸ë¶€ ì˜µì…˜' ë“± ê°€ê²©ì— ì˜í–¥ì„ ë¯¸ì¹  ìˆ˜ ìˆëŠ” ì¤‘ìš” í”¼ì²˜ê°€ ëˆ„ë½ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ë°ì´í„°ê°€ ì¶”ê°€ëœë‹¤ë©´ ëª¨ë¸ì˜ ì˜ˆì¸¡ ì„±ëŠ¥ì´ ë”ìš± ê°œì„ ë  ê²ƒì…ë‹ˆë‹¤.

ì´ë²ˆ í”„ë¡œì íŠ¸ì—ì„œëŠ” ê¸°ë³¸ íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í–ˆì§€ë§Œ, í–¥í›„ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹(Hyperparameter Tuning)ì„ í†µí•´ ëª¨ë¸ì„ ë”ìš± ì •êµí•˜ê²Œ ìµœì í™”í•˜ì—¬ ì„±ëŠ¥ì„ ì¶”ê°€ë¡œ í–¥ìƒì‹œí‚¬ ê³„íšì…ë‹ˆë‹¤.

ğŸ‡¬ğŸ‡§ README.md (English Version)
(This is the English version for your portfolio. You can copy and paste this as well.)

ğŸš— Used Car Price Prediction Model Project
1. Overview
This is a foundational personal project aimed at building basic skills in data analysis and machine learning. Using a used car dataset from Kaggle, this project covers the end-to-end process of developing and evaluating a regression model to predict vehicle prices based on various features.

The primary goal is not just to build a model, but to experience and understand the entire machine learning pipeline, from data cleaning and feature engineering to modeling and performance evaluation. This project serves to demonstrate problem-solving abilities and data-driven logical thinking.

2. Environment & Data
Environment: Python, Jupyter Notebook, Google Colab

Core Libraries: Pandas, NumPy, Matplotlib, Seaborn, Scikit-learn

Dataset: Kaggle - Used Car Price Prediction (Insert the exact link to the dataset you used here)

3. Methodology / Project Pipeline
This project was conducted through the following systematic steps:

Exploratory Data Analysis (EDA): Investigated the basic structure and statistical properties of the data, and explored relationships between variables through visualization.

Data Preprocessing: Handled missing values and converted categorical features (text-based) into numerical formats suitable for the model.

Feature Engineering: Created new, meaningful derivative features, such as 'Car_Age', from existing ones to improve model performance.

Modeling: Trained and compared various regression models, including Linear Regression and Random Forest.

Evaluation: Objectively assessed the predictive performance of each model using RMSE (Root Mean Squared Error) to select the optimal model.

4. Results
Model Performance Comparison
After training several models, the following performance metrics were observed:

Model	RMSE (Root Mean Squared Error)
Linear Regression	(e.g., 4.51)
Random Forest	(e.g., 2.87)
Gradient Boosting	(e.g., 2.55)

Sheetsë¡œ ë‚´ë³´ë‚´ê¸°
The analysis concluded that the Gradient Boosting model was optimal, exhibiting the lowest Root Mean Squared Error (RMSE).

Key Feature Importance
The feature importance plot from the final Gradient Boosting model is shown below. It indicates that features like 'Power' and 'Car_Age' have a significant impact on price prediction.

(Capture the feature importance graph from your notebook and insert the image here)

5. Conclusion & What I Learned
Conclusion
This project successfully built a predictive model and confirmed, through a data-driven approach, that key features like mileage, year, and engine power significantly influence used car prices. It was empirically verified that feature engineering, particularly the creation of new variables like 'Car_Age', plays a crucial role in enhancing model performance.

What I Learned & Future Work
What I Learned:

I gained a practical understanding of how critical and time-consuming data preprocessing (handling missing values, encoding) is in a real-world analysis.

I learned that a deep understanding of the data through EDA and feature engineering must precede modeling to achieve meaningful performance.

I realized the importance of systematically documenting the project process through tools like GitHub for clarity and reproducibility.

Limitations & Future Work:

The current dataset lacks important features that could affect price, such as accident history, color, and specific options. Including this data would likely improve model accuracy.

This project used default model parameters. Future work will involve hyperparameter tuning to further optimize the model and enhance its predictive power.